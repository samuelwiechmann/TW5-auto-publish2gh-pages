created: 20180307163520773
modified: 20180320125236600
tags: physicalconcept 2 topic concept
title: Information
tmap.edges: {"ccf0e55a-8f7d-4fa7-b396-5ecf5c6d4437":{"to":"2a98efdb-39e0-4077-9c68-2bafe3c4a9b3","type":"converts to"},"a1518b10-6069-48b5-9115-c1dfef63d52c":{"to":"318382e5-829e-4938-9ac1-ef5fbe61c151","type":"equivalent to"}}
tmap.id: 7cc4b9b1-fc88-4281-a2d7-e27ec6339b00
type: text/vnd.tiddlywiki

Information is any entity or form that resolves uncertainty or provides the answer to a question of some kind.

The uncertainty of an event is measured by its probability of occurrence and is inversely proportional to that. The more uncertain an event, the more information is required to resolve uncertainty of that event. The bit is a typical unit of information, but other units such as the nat may be used. For example, the information encoded in one "fair" coin flip is log2(2/1) = 1 bit, and in two fair coin flips is log2(4/1) = 2 bits.

From the stance of [[Information theory]], information is taken as an ordered sequence of symbols from an alphabet, say an input alphabet χ, and an output alphabet ϒ.

Information has a well-defined meaning in physics. In 2003 J. D. Bekenstein claimed that a growing trend in physics was to define the physical world as being made up of information itself.
https://en.wikipedia.org/wiki/Physical_information

https://en.wikipedia.org/wiki/Information

A good read on this topic is Cesar Hidalgos book "How information grows", deutscher Titel: "Wachstum geht anders"